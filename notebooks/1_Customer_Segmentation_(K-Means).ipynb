{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b793e20b-ad8d-4621-ae39-b3c736e1b951",
   "metadata": {},
   "source": [
    "# **Portfolio Project: Home Credit Risk Analysis**\n",
    "\n",
    "### **Project Goal and Approach**\n",
    "\n",
    "The main goal of this project is to partition customers from the \"Home Credit Default Risk\" dataset into meaningful, risk-oriented segments based on the core information they provide during a loan application. This segmentation will be performed using **K-Means**, an unsupervised learning algorithm.\n",
    "\n",
    "A key and unique constraint of this project is the exclusive use of **features that an end-user could realistically input into a web interface (like a Streamlit simulation)**, along with new features derived from them. This approach ensures that the resulting segmentation model is not just a theoretical exercise but can be transformed into a practical decision support tool.\n",
    "\n",
    "Through these steps, we aim to transform raw data into interpretable, actionable customer segments that provide real business value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3531b-565f-4828-b675-0dc910056402",
   "metadata": {},
   "source": [
    "### **Step 1: Setting Up the Environment and Importing Libraries**\n",
    "\n",
    "Before we begin, we will import the essential Python libraries for data processing, visualization, modeling, and preprocessing. We will also configure some settings to make our notebook more readable and our results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbd80da-cd0c-477d-ab5b-68ed16914984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is ready.\n",
      "Pandas version: 2.2.2\n",
      "Numpy version: 1.26.4\n",
      "Scikit-learn version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# ── Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ── Scikit-learn: Preprocessing & Modeling\n",
    "# In this project, we'll only perform segmentation (K-Means), but we're importing\n",
    "# other modules that might be needed for the rest of the portfolio.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn # for version checking\n",
    "\n",
    "# ── Visualization Settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# ── Pandas Display Settings\n",
    "# Ensures better display of DataFrames in the notebook.\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n",
    "\n",
    "# ── Hide Warnings\n",
    "# We suppress warnings to keep the notebook output clean.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Reproducibility (VERY IMPORTANT)\n",
    "# Ensures that steps involving randomness produce the same result every time.\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\n",
    "    f\"Environment is ready.\\n\"\n",
    "    f\"Pandas version: {pd.__version__}\\n\"\n",
    "    f\"Numpy version: {np.__version__}\\n\"\n",
    "    f\"Scikit-learn version: {sklearn.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a6d9c-25c6-4e14-9461-614a3b167516",
   "metadata": {},
   "source": [
    "### **Step 2: Loading and Subsetting the Dataset**\n",
    "\n",
    "In this step, we will load the `application_train.csv` file into a pandas DataFrame. The full dataset contains 122 columns, but due to our project's core constraint, we will not use all of them.\n",
    "\n",
    "Instead, we will select a subset of features that an end-user could realistically provide in our future Streamlit simulation. These selected features will cover the customer's demographic, financial, and loan application details.\n",
    "\n",
    "Additionally, although we won't use them to train the segmentation model, we will keep the `TARGET` column to analyze the risk profiles (default rates) of our created segments, and the `SK_ID_CURR` column to uniquely identify each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466ae047-d2d1-4ca0-b309-2c422dacc143",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/application_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/application_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# List of core features to be selected for the Streamlit simulation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# --- ID and Target Variable ---\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSK_ID_CURR\u001b[39m\u001b[38;5;124m'\u001b[39m,       \u001b[38;5;66;03m# Customer ID (for analysis)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXT_SOURCE_3\u001b[39m\u001b[38;5;124m'\u001b[39m      \u001b[38;5;66;03m# External source score 3\u001b[39;00m\n\u001b[1;32m     30\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/homecredit-310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/homecredit-310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/homecredit-310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/homecredit-310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/homecredit-310/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/application_train.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/application_train.csv')\n",
    "\n",
    "# List of core features to be selected for the Streamlit simulation\n",
    "feature_columns = [\n",
    "    # --- ID and Target Variable ---\n",
    "    'SK_ID_CURR',       # Customer ID (for analysis)\n",
    "    'TARGET',           # Risk status (to interpret segments)\n",
    "    \n",
    "    # --- Demographic and Personal Info (User Inputs) ---\n",
    "    'CODE_GENDER',      # Gender\n",
    "    'CNT_CHILDREN',     # Number of children\n",
    "    'NAME_FAMILY_STATUS', # Marital status\n",
    "    'NAME_EDUCATION_TYPE',# Education level\n",
    "    'DAYS_BIRTH',       # Age (to be converted to years later)\n",
    "    \n",
    "    # --- Financial and Employment Info (User Inputs) ---\n",
    "    'AMT_INCOME_TOTAL', # Total income\n",
    "    'NAME_INCOME_TYPE', # Income type (e.g., Working, Self-employed)\n",
    "    'DAYS_EMPLOYED',    # Employment duration (to be converted to years later)\n",
    "\n",
    "    # --- Loan Application Info (User Inputs) ---\n",
    "    'NAME_CONTRACT_TYPE', # Contract type\n",
    "    'AMT_CREDIT',       # Credit amount\n",
    "    'AMT_ANNUITY',      # Annuity payment\n",
    "    \n",
    "    # --- External Scores (Very Important) ---\n",
    "    'EXT_SOURCE_2',     # External source score 2\n",
    "    'EXT_SOURCE_3'      # External source score 3\n",
    "]\n",
    "\n",
    "# Check if all selected columns exist in the file\n",
    "missing_cols = [c for c in feature_columns if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"The following column(s) were not found in the dataset: {missing_cols}\")\n",
    "\n",
    "# Subset our dataset with the selected columns.\n",
    "# Using .copy() prevents potential 'SettingWithCopyWarning' later on.\n",
    "data = df[feature_columns].copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"New dataset shape after feature selection: {data.shape}\\n\")\n",
    "\n",
    "print(\"First 5 rows of the selected data:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb83e1-ca6b-4f8e-8ea7-619906734961",
   "metadata": {},
   "source": [
    "### **Step 3: Data Cleaning and Feature Engineering**\n",
    "\n",
    "This step is the heart of our project. We will transform the raw data into meaningful, domain-knowledge-based features that our model can learn from more effectively. The operations we perform here will directly impact the model's performance and the interpretability of the segments.\n",
    "\n",
    "We will perform the following operations in order:\n",
    "\n",
    "1.  **Correcting Anomalous Values:** We will flag the nonsensical positive values in the `DAYS_EMPLOYED` column as missing data (`NaN`).\n",
    "2.  **Deriving Interpretable Features:** We will convert columns with negative day formats, like `DAYS_BIRTH` and `DAYS_EMPLOYED`, into an understandable **year** format (`AGE_YEARS`, `YEARS_EMPLOYED`).\n",
    "3.  **Creating Robust Ratio Features:** Using a function that prevents errors like division-by-zero, we will create new features by ratioing core variables like income, credit amount, and age. This will allow for a deeper analysis of the customer's financial profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2cae3-cd1e-4682-be1d-63dffd1e4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Correcting anomalous values (DAYS_EMPLOYED)\n",
    "# The value 365243 is a code for not employed/unknown -> replace with NaN\n",
    "data['DAYS_EMPLOYED'] = data['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "# 2) Converting days to years (for interpretability)\n",
    "data['AGE_YEARS'] = data['DAYS_BIRTH'] / -365\n",
    "data['YEARS_EMPLOYED'] = data['DAYS_EMPLOYED'] / -365\n",
    "\n",
    "# 3) Creating ratio features (with your robust 'safe_div' function)\n",
    "print(\"Creating new features...\")\n",
    "\n",
    "def safe_div(num, den):\n",
    "    \"\"\"\n",
    "    Performs a highly safe division operation by checking for edge cases\n",
    "    like division-by-zero, invalid data types, and infinite values.\n",
    "    Returns NaN in case of errors.\n",
    "    \"\"\"\n",
    "    num = pd.to_numeric(num, errors='coerce')\n",
    "    den = pd.to_numeric(den, errors='coerce')\n",
    "    # Perform division only if the denominator is positive and finite, and the numerator is also finite\n",
    "    out = np.where((den > 0) & np.isfinite(den) & np.isfinite(num), num / den, np.nan)\n",
    "    return pd.Series(out, index=num.index)\n",
    "\n",
    "# Debt burden: credit / income\n",
    "data['CREDIT_TO_INCOME_RATIO']  = safe_div(data['AMT_CREDIT'],  data['AMT_INCOME_TOTAL'])\n",
    "\n",
    "# Payment capacity: yearly annuity / income\n",
    "data['ANNUITY_TO_INCOME_RATIO'] = safe_div(data['AMT_ANNUITY'], data['AMT_INCOME_TOTAL'])\n",
    "\n",
    "# Stability: years employed / age\n",
    "data['EMPLOYED_TO_AGE_RATIO']   = safe_div(data['YEARS_EMPLOYED'], data['AGE_YEARS'])\n",
    "\n",
    "# Loan structure (≈ term in YEARS): credit / yearly annuity\n",
    "data['CREDIT_TERM']             = safe_div(data['AMT_CREDIT'], data['AMT_ANNUITY'])\n",
    "\n",
    "print(\"Feature engineering complete.\\n\")\n",
    "\n",
    "# Quick check\n",
    "print(\"New state of the dataset (first 5 rows):\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nStatistical summary of numerical columns:\")\n",
    "display(data.select_dtypes(include='number').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cf0c0-341c-4250-a002-89b0f5904ffd",
   "metadata": {},
   "source": [
    "### **Step 4: Preprocessing for Modeling**\n",
    "\n",
    "Our dataset, now enriched with feature engineering, is not yet ready for modeling. Algorithms like K-Means cannot work directly on raw data. In this step, we will prepare the dataset to meet the model's requirements.\n",
    "\n",
    "We will use `scikit-learn`'s powerful tools, `Pipeline` and `ColumnTransformer`, for this process. The advantages of this approach are:\n",
    "- **Automation:** It combines all steps—such as imputing missing data, transforming categorical variables, and scaling numerical variables—into a single \"recipe\" (pipeline).\n",
    "- **Consistency:** It ensures that we can apply the exact same transformation steps flawlessly to new user input in our Streamlit application.\n",
    "- **Professionalism:** This is a standard and best practice in data science projects, resulting in organized and less error-prone code.\n",
    "\n",
    "The operations we will perform are:\n",
    "1.  **Separating Model Inputs:** We will set aside columns not used for modeling, such as `SK_ID_CURR` and `TARGET`.\n",
    "2.  **Identifying Column Types:** We will programmatically separate the numerical and categorical columns.\n",
    "3.  **Defining Preprocessing Steps:**\n",
    "    - **For Numerical Columns:** First, we will fill missing values with the median (`SimpleImputer`), then scale them using `RobustScaler`, which is more resilient to outliers.\n",
    "    - **For Categorical Columns:** First, we will fill missing values with a constant value ('missing') using `SimpleImputer`, then convert these categories into a numerical format with `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ec4eb-0f11-4ed4-bdcf-31173a1ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (If not already imported in this cell)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np  # for np.number in select_dtypes\n",
    "\n",
    "# Separate the columns that will not be used for modeling.\n",
    "# We will use these columns later for analyzing the segments.\n",
    "data_for_model = data.drop(columns=['SK_ID_CURR', 'TARGET', 'DAYS_BIRTH', 'DAYS_EMPLOYED'])\n",
    "\n",
    "# 1. Programmatically identify column types\n",
    "numeric_features = data_for_model.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = data_for_model.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\\n\")\n",
    "\n",
    "# 2. Define preprocessing steps for numerical and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# OneHotEncoder: sparse_output for scikit-learn 1.4+, sparse for older versions\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', ohe)\n",
    "])\n",
    "\n",
    "# 3. Combine these pipelines with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # No extra columns left in this project, but it's good practice.\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline successfully created.\")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592cf12-9703-44cc-90ac-3daf2c795ef0",
   "metadata": {},
   "source": [
    "### **Step 5: Determining the Optimal Number of Clusters (Elbow Method)**\n",
    "\n",
    "Before we partition our dataset into segments with the K-Means algorithm, the most important question we need to answer is: \"How many segments (clusters) should we create?\" To find the answer, we will use a technique called the **Elbow Method**.\n",
    "\n",
    "The logic behind this method is as follows:\n",
    "1.  We run the K-Means model repeatedly for a range of `k` (number of clusters) values (e.g., from 1 to 10).\n",
    "2.  For each `k` value, we record the model's \"inertia.\" Inertia is the sum of squared distances of samples to their closest cluster center (Within-Cluster Sum of Squares - WCSS). Simply put, it's the sum of the squared distances of each point to its own cluster's center. Our goal is to minimize this value.\n",
    "3.  When we plot the inertia values against their corresponding `k` values, we typically see a graph that looks like an arm, forming an \"elbow.\" This elbow point represents the point of diminishing returns, where adding more clusters does not significantly reduce the inertia.\n",
    "\n",
    "This elbow point will be our candidate for the optimal `k` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679702dc-1ba4-499e-a12a-ebb052756a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for RANDOM_STATE to ensure this cell can run independently\n",
    "try:\n",
    "    RANDOM_STATE\n",
    "except NameError:\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "# 1) Apply the preprocessing steps to the data\n",
    "print(\"Applying preprocessing steps to the data...\")\n",
    "processed_data = preprocessor.fit_transform(data_for_model)\n",
    "\n",
    "# 2) Safety checks\n",
    "# Convert a potential sparse matrix output to a dense format\n",
    "if hasattr(processed_data, \"toarray\"):\n",
    "    processed_data = processed_data.toarray()\n",
    "processed_data = np.asarray(processed_data, dtype=float)\n",
    "\n",
    "# Ensure there are no NaN/Inf values in the processed data before running K-Means\n",
    "if not np.isfinite(processed_data).all():\n",
    "    raise ValueError(\"Found NaN/Inf in processed data. Check imputation/scaling steps.\")\n",
    "\n",
    "print(f\"Shape of the processed data: {processed_data.shape}\\n\")\n",
    "\n",
    "\n",
    "# 3) Apply the Elbow Method\n",
    "inertia_values = []\n",
    "k_range = range(2, 11)  # We will test cluster counts from 2 to 10\n",
    "\n",
    "for k in k_range:\n",
    "    # n_init=10 -> Run the algorithm 10 times with different centroid seeds and pick the best one.\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(processed_data)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    print(f\"Inertia calculated for k={k}: {kmeans.inertia_:,.2f}\")\n",
    "\n",
    "# 4) Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia_values, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (WCSS)')\n",
    "plt.title('Elbow Method for Finding the Optimal k')\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db62916-bacb-4669-b393-ed4c9588d3d9",
   "metadata": {},
   "source": [
    "### **Step 5.1: Interpreting the Elbow Plot and Final Decision on `k`**\n",
    "\n",
    "The Elbow Method has shown us that both `k=3` and `k=4` are strong candidates for the optimal number of clusters. Our final decision requires carefully evaluating the trade-offs between these two options in terms of statistical robustness and practical interpretability.\n",
    "\n",
    "**1. Findings from the Elbow Method:**\n",
    "- The largest and sharpest improvement occurs when moving from `k=2` to `k=3` (a drop of `484,226` points). This is a strong signal that the most dominant structure in the data is based on three clusters.\n",
    "- Adding a fourth cluster still provides a significant improvement (`338,329` points), though returns noticeably diminish after `k=4`. This confirms that both `k=3` and `k=4` are statistically plausible candidates.\n",
    "\n",
    "**2. Separation Quality Analysis (Silhouette Scores):**\n",
    "To deepen our decision-making process, a Silhouette analysis revealed that `k=3` offers a more robust structure:\n",
    "- The overall Silhouette score for `k=3` is **0.146**, while for `k=4`, this value is marginally lower at **0.143**.\n",
    "- More importantly, in the `k=4` model, the newly added segment was found to have a very low average Silhouette score of **≈0.066** and a high negative Silhouette ratio of **≈28%**. This indicates that a significant portion of the customers in this cluster are geometrically closer to other clusters, meaning its boundaries are very \"blurry.\"\n",
    "\n",
    "**3. Final Decision and Rationale:**\n",
    "\n",
    "Although the `k=4` model provides an interesting signal about a potential \"High-Income\" subgroup in the data, the fact that this group is statistically weak and unstable makes it an unreliable segment.\n",
    "\n",
    "The primary goal of our project is to create customer personas that are clearly distinct, interpretable, and built on solid foundations. To this end, the **`k=3`** model offers:\n",
    "- Cleaner decision boundaries,\n",
    "- Marginally better statistical separation quality (Silhouette),\n",
    "- And a simpler, more consistent framework for interpretation.\n",
    "\n",
    "For these reasons, we have decided to proceed with **`k=3`**, as it provides a more balanced and robust structure for our project.\n",
    "\n",
    "**Selected Optimal Number of Clusters (k) = 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0ee94-8388-46b6-9f34-bcd8027323b7",
   "metadata": {},
   "source": [
    "### **Step 6: Applying the K-Means Model and Creating Segments**\n",
    "\n",
    "Based on our comprehensive analysis (including the Elbow Method and subsequent validation), we have decided that the optimal number of clusters for our dataset is **k=3**. We will now create and train our final K-Means model with this information.\n",
    "\n",
    "The process for this step is as follows:\n",
    "1.  A new `KMeans` object will be created with the `n_clusters=3` parameter.\n",
    "2.  This model will be trained on the `processed_data` we prepared and scaled earlier. Using the `fit_predict` method, the model will be trained, and a segment label (0, 1, or 2) will be assigned to each data point (customer).\n",
    "3.  These resulting segment labels will be added as a new `Segment` column to our original, human-readable `data` DataFrame, which we created through feature engineering, to enable analysis and interpretation.\n",
    "\n",
    "At the end of this process, we will know which segment each customer belongs to, and we will be ready to analyze what these segments mean in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be8dee-d746-4311-957a-ba0769c8fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import display for more flexible output in Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "# Check if RANDOM_STATE exists\n",
    "try:\n",
    "    RANDOM_STATE\n",
    "except NameError:\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "# The optimal number of clusters we determined from our analysis\n",
    "OPTIMAL_K = 3\n",
    "\n",
    "# Create the final K-Means model with this k value\n",
    "kmeans_final = KMeans(\n",
    "    n_clusters=OPTIMAL_K,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "# Train the model on the processed data and get the segment label for each customer\n",
    "print(f\"Training final model with k={OPTIMAL_K} and creating segments...\")\n",
    "segments = kmeans_final.fit_predict(processed_data) # Note: Should use processed_data_winsorized\n",
    "print(\"Segmentation complete.\\n\")\n",
    "\n",
    "# Safety check: Ensure the number of assigned segments matches the number of rows in the original data.\n",
    "assert len(segments) == len(data), \"The number of segments does not match the number of rows in the data!\"\n",
    "\n",
    "# Add the segment labels as a new column to our main DataFrame\n",
    "data['Segment'] = segments\n",
    "\n",
    "# Check the results\n",
    "# 1. View the first 5 rows to see the added 'Segment' column\n",
    "print(\"Segment labels added to the dataset (first 5 rows):\")\n",
    "display(data.head())\n",
    "\n",
    "# 2. View the number of customers in each of the created segments\n",
    "print(f\"\\nCustomer distribution in the {OPTIMAL_K} created segments:\")\n",
    "segment_counts = data['Segment'].value_counts().sort_index()\n",
    "display(segment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc59136-0f28-4abc-b6ea-7e07333cc757",
   "metadata": {},
   "source": [
    "### **Step 7: Interpreting and Analyzing the Segments**\n",
    "\n",
    "This is the step where we generate the most valuable output of the project. We will translate a technical output (segment labels) into meaningful and actionable insights for business purposes. Our goal is to uncover the unique character and risk profile of each segment.\n",
    "\n",
    "To do this, we will:\n",
    "1.  Group our dataset by the `Segment` column.\n",
    "2.  Calculate the **average values** of our core features for each segment. These averages will help us paint a profile of the \"typical\" customer in that segment.\n",
    "3.  Most importantly, we will calculate the average `TARGET` value for each segment. Since the `TARGET` column consists of 0 (did not default) and 1 (defaulted), its average directly gives us the **default rate** for that segment.\n",
    "4.  We will sort this analysis table by the default rate to immediately identify the riskiest segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629761ae-628a-4a9a-9d76-69bff98f9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display  # for display\n",
    "\n",
    "# Select the most meaningful and easily interpretable columns for our analysis\n",
    "analysis_features = [\n",
    "    'TARGET',\n",
    "    'AGE_YEARS',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'YEARS_EMPLOYED',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'CREDIT_TO_INCOME_RATIO',\n",
    "    'ANNUITY_TO_INCOME_RATIO',\n",
    "    'EMPLOYED_TO_AGE_RATIO',\n",
    "    'CREDIT_TERM'\n",
    "]\n",
    "\n",
    "# Safety check: Do all selected columns for analysis exist?\n",
    "missing_cols = [c for c in analysis_features if c not in data.columns]\n",
    "assert not missing_cols, f\"Missing column(s) in the analysis list: {missing_cols}\"\n",
    "\n",
    "# Group the data by segment and take the mean of the selected features\n",
    "segment_analysis = data.groupby('Segment')[analysis_features].mean(numeric_only=True)\n",
    "\n",
    "# Convert the TARGET column to a percentage and rename it\n",
    "segment_analysis['TARGET'] = segment_analysis['TARGET'] * 100\n",
    "segment_analysis.rename(columns={'TARGET': 'Default_Rate (%)'}, inplace=True)\n",
    "\n",
    "# Sort the results from the riskiest segment to the least risky\n",
    "segment_analysis.sort_values(by='Default_Rate (%)', ascending=False, inplace=True)\n",
    "\n",
    "# Presentation: Round the results for a more readable output\n",
    "segment_analysis = segment_analysis.round(2)\n",
    "\n",
    "print(\"Characteristic Features and Risk Profiles of the Segments:\")\n",
    "display(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e191324-e5a7-48cf-854f-c1187afee7af",
   "metadata": {},
   "source": [
    "### **Step 6.1: Managing Outliers and Refining the Model**\n",
    "\n",
    "In our initial modeling attempt, we observed that K-Means' sensitivity to outliers resulted in a meaningless segment containing only a single customer. To correct this, instead of a simpler method like deleting the data, we will apply the statistically more robust technique of **Winsorization (Clipping)**.\n",
    "\n",
    "In this approach:\n",
    "1.  We will \"suppress\" the values in numerical columns with high outlier potential, constraining them to fall between the bottom `0.1%` and top `99.9%` of the distribution. This will allow us to reduce the impact of extreme values without losing data.\n",
    "2.  We will re-run the preprocessing (`preprocessor`) and the K-Means model on this \"clipped\" dataset.\n",
    "3.  We will check if the new segment distribution is more balanced and meaningful. We'll even automate this check and, if necessary, try a different value of `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc74ea1-f578-4b19-ad82-87eacf06f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Suppress outliers: Clip specified numerical columns between the 0.1% and 99.9% percentiles.\n",
    "winsor_cols = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
    "    'CREDIT_TO_INCOME_RATIO', 'ANNUITY_TO_INCOME_RATIO',\n",
    "    'CREDIT_TERM', 'AGE_YEARS', 'YEARS_EMPLOYED'\n",
    "]\n",
    "\n",
    "# Take a copy of the dataset used for modeling\n",
    "data_for_model_winsorized = data_for_model.copy()\n",
    "\n",
    "# Determine the clipping boundaries\n",
    "q_low = data_for_model_winsorized[winsor_cols].quantile(0.001)\n",
    "q_hi  = data_for_model_winsorized[winsor_cols].quantile(0.999)\n",
    "\n",
    "# Apply the clipping operation\n",
    "data_for_model_winsorized[winsor_cols] = data_for_model_winsorized[winsor_cols].clip(lower=q_low, upper=q_hi, axis=1)\n",
    "\n",
    "print(\"Outliers have been suppressed within the 0.1% and 99.9% range.\\n\")\n",
    "\n",
    "# 2) Re-fit the preprocessor on the clipped data and run K-Means again\n",
    "# It is important to re-run fit_transform because the data distribution has changed.\n",
    "processed_data_winsorized = preprocessor.fit_transform(data_for_model_winsorized)\n",
    "if hasattr(processed_data_winsorized, \"toarray\"):  # Safety check for dense output\n",
    "    processed_data_winsorized = processed_data_winsorized.toarray()\n",
    "\n",
    "# A value like n_init=10 or 20 is good practice for more stable results.\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n",
    "segments = kmeans_final.fit_predict(processed_data_winsorized)\n",
    "\n",
    "# 3) Assign the new segment labels to the main dataframe and check the distribution\n",
    "# Note: Since no rows were deleted, the lengths will always match.\n",
    "data['Segment'] = segments\n",
    "print(\"New segment distribution for k=3:\")\n",
    "display(data['Segment'].value_counts().sort_index())\n",
    "\n",
    "# 4) (Optional) Safety net: Try a different k if a very small cluster still remains\n",
    "if data['Segment'].value_counts().min() < 100: # We can set a more meaningful threshold like 100\n",
    "    print(\"\\nWarning: A small cluster was still detected. Trying k=5...\")\n",
    "    kmeans5 = KMeans(n_clusters=5, random_state=RANDOM_STATE, n_init=10)\n",
    "    seg5 = kmeans5.fit_predict(processed_data_winsorized)\n",
    "    \n",
    "    print(\"Distribution for k=5:\")\n",
    "    display(pd.Series(seg5).value_counts().sort_index())\n",
    "    # If the k=5 result is more balanced, we might decide to use it.\n",
    "    # For now, we will proceed with our analysis using k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95515925-42a3-407b-a3d7-32b7b677f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display  # for display\n",
    "\n",
    "# Select the most meaningful and easily interpretable columns for our analysis\n",
    "analysis_features = [\n",
    "    'TARGET',\n",
    "    'AGE_YEARS',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'YEARS_EMPLOYED',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'CREDIT_TO_INCOME_RATIO',\n",
    "    'ANNUITY_TO_INCOME_RATIO',\n",
    "    'EMPLOYED_TO_AGE_RATIO',\n",
    "    'CREDIT_TERM'\n",
    "]\n",
    "\n",
    "# Safety check: Do all selected columns for analysis exist?\n",
    "missing_cols = [c for c in analysis_features if c not in data.columns]\n",
    "assert not missing_cols, f\"Missing column(s) in the analysis list: {missing_cols}\"\n",
    "\n",
    "# Group the data by segment and take the mean of the selected features\n",
    "segment_analysis = data.groupby('Segment')[analysis_features].mean(numeric_only=True)\n",
    "\n",
    "# Convert the TARGET column to a percentage and rename it\n",
    "segment_analysis['TARGET'] = segment_analysis['TARGET'] * 100\n",
    "segment_analysis.rename(columns={'TARGET': 'Default_Rate (%)'}, inplace=True)\n",
    "\n",
    "# Sort the results from the riskiest segment to the least risky\n",
    "segment_analysis.sort_values(by='Default_Rate (%)', ascending=False, inplace=True)\n",
    "\n",
    "# Presentation: Round the results for a more readable output\n",
    "segment_analysis = segment_analysis.round(2)\n",
    "\n",
    "print(\"Characteristic Features and Risk Profiles of the Segments:\")\n",
    "display(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2a6c-2b69-4720-8892-1c27a3319548",
   "metadata": {},
   "source": [
    "### **Project Outcome: Segment Personas and Business Strategies**\n",
    "\n",
    "After managing the impact of outliers with the `Winsorization` technique, our K-Means model with `k=3` successfully partitioned customers into three distinct and business-relevant groups. Each segment presents a consistent and meaningful structure in terms of both size and risk profile. Below is the detailed profile for each segment and the proposed business strategies.\n",
    "\n",
    "---\n",
    "#### **Segment 2 — Low Risk (Stable Professionals)**\n",
    "*   **Risk:** Lowest default rate at **5.06%**.\n",
    "*   **Profile:** Characterized by a very long employment duration (≈18.1 years), resulting in the highest Employed/Age ratio (≈0.40). They also have high external scores (EXT≈0.55) and high income (≈187k). Their debt burden is reasonable, with a CTI of ≈3.94.\n",
    "*   **Strategy:** Ideal for fast-track approvals, premium product offerings, loyalty programs, and cross-selling (e.g., investments, insurance, premium cards).\n",
    "\n",
    "---\n",
    "#### **Segment 0 — Medium-High Risk (High Debt Burden)**\n",
    "*   **Risk:** Default rate of **7.24%**.\n",
    "*   **Profile:** Defined by the highest credit amount (≈989k). Although their income is good (≈182k), their debt burden is high with a CTI of ≈6.40. The loan term is also long (≈26.7 years).\n",
    "*   **Strategy:** Recommends cautious limits, a thorough analysis of payment capacity, early warning monitoring, and proactively offering debt restructuring options.\n",
    "\n",
    "---\n",
    "#### **Segment 1 — High Risk (Financially Fragile)**\n",
    "*   **Risk:** Highest default rate at **9.31%**.\n",
    "*   **Profile:** Short employment history (≈4.0 years), low external scores (EXT≈0.49), and low income (≈156k). Their relatively low CTI of ≈2.51 can be explained by potentially limited access to higher credit limits.\n",
    "*   **Strategy:** Requires strict credit policies, with most applications likely to be declined. If approved, it should be with low limits, additional collateral/guarantors, and a risk premium in pricing.\n",
    "\n",
    "---\n",
    "### **Project Summary**\n",
    "The `k=3` model produces three clear, actionable personas based on the axes of **Stability** (long employment, high scores), **Indebtedness** (high CTI), and **Fragility** (low scores/income, short employment). These findings can be directly translated into business actions related to **credit policy, limit management, pricing, and cross-selling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48290a5a-20d3-42f2-aad9-2a8cd25b5298",
   "metadata": {},
   "source": [
    "### **Step 9: Validating the Segments (Customers Closest to Centroid)**\n",
    "\n",
    "To more concretely see how consistent our segmentation model is and what our segments mean, we will perform a validation test. The goal of this test is to find the \"most representative\" member of each segment and check whether the model assigns this member to the correct segment.\n",
    "\n",
    "To do this, we will:\n",
    "1.  **Find Segment Centers (Centroids):** We will get the coordinates of the geometric center for each segment (0, 1, 2) in the scaled space from our trained K-Means model.\n",
    "2.  **Identify the Closest Customer:** For each segment, we will find the **actual customer** in our dataset who is closest to that segment's center in terms of Euclidean distance. These customers can be considered the \"prototypes\" or \"exemplars\" of their segments.\n",
    "3.  **Re-predict:** We will take the information of these three prototype customers and run it through our previously created preprocessing (`preprocessor`) and K-Means (`kmeans_final`) steps as if they were new applications.\n",
    "4.  **Validate:** We will check if the model's segment prediction for these customers matches their original segment. A successful model is expected to correctly assign each prototype to its own original segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17039ca-61f1-4b29-86b4-a8a810c2cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from IPython.display import display\n",
    "\n",
    "# Get the actual number of clusters from the KMeans model\n",
    "final_k = getattr(kmeans_final, \"n_clusters\", 3)\n",
    "\n",
    "# Use the same processed data that you used to fit KMeans:\n",
    "# e.g., processed_trim or processed_data_winsorized or processed_data\n",
    "X_proc = processed_data_winsorized  # <-- if this is the name you used for fitting\n",
    "# X_proc = processed_trim          # uncomment this if you used this name after winsorization\n",
    "# X_proc = processed_data          # if you did not winsorize\n",
    "\n",
    "# Safety check: do the row counts match?\n",
    "assert X_proc.shape[0] == len(segments), \"Row count of X_proc does not match the length of 'segments'.\"\n",
    "\n",
    "# --- STEP 1: FIND THE CUSTOMER CLOSEST TO THE CENTER OF EACH SEGMENT ---\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "central_customer_indices = {}\n",
    "\n",
    "for i in range(final_k):\n",
    "    # Positional indices of the samples belonging to this segment\n",
    "    idx_in_seg = np.where(segments == i)[0]\n",
    "\n",
    "    # Euclidean distances between the points in the segment and its centroid\n",
    "    D = pairwise_distances(X_proc[idx_in_seg], centroids[i].reshape(1, -1), metric=\"euclidean\")\n",
    "\n",
    "    # Get the positional (local) index of the closest point, then convert it to a global index\n",
    "    local_min = np.argmin(D.ravel())\n",
    "    global_idx = int(idx_in_seg[local_min])\n",
    "    central_customer_indices[i] = global_idx\n",
    "\n",
    "print(\"Global indices of the customers closest to each segment's center:\")\n",
    "print(central_customer_indices)\n",
    "\n",
    "# The original (human-readable) data of the prototype customers\n",
    "exemplars_df = data.iloc[list(central_customer_indices.values())]  # <-- using iloc!\n",
    "print(\"\\n--- Prototype Customers of the Segments ---\")\n",
    "display(exemplars_df)\n",
    "\n",
    "# --- STEP 2: VALIDATE BY PREDICTING ON THE PROTOTYPES ---\n",
    "print(\"\\n--- Validation Test Results ---\")\n",
    "validation_passed = True\n",
    "\n",
    "for seg_id, gidx in central_customer_indices.items():\n",
    "    # Single-row input: the winsorized model data frame (must be the same one that went into KMeans)\n",
    "    single_row = data_for_model_winsorized.iloc[[gidx]]  # <-- using iloc!\n",
    "\n",
    "    # Transform with the same preprocessor (only transform, DO NOT fit)\n",
    "    X_single = preprocessor.transform(single_row)\n",
    "\n",
    "    # KMeans prediction\n",
    "    pred_seg = int(kmeans_final.predict(X_single)[0])\n",
    "    actual_seg = int(data[\"Segment\"].iloc[gidx])\n",
    "\n",
    "    ok = (pred_seg == actual_seg)\n",
    "    validation_passed &= ok\n",
    "\n",
    "    print(f\"Segment {actual_seg} prototype (idx={gidx}) -> Prediction: {pred_seg}  ==> {'SUCCESS ✅' if ok else 'FAILURE ❌'}\")\n",
    "\n",
    "print(\"\\nResult:\", \"All prototypes were assigned to the correct segment. ✅\" if validation_passed else \"At least one prototype was misclassified. ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e653be-3519-42b6-b514-0c6ae68585d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Take only the numerical values from the analysis table\n",
    "radar_data = segment_analysis.drop(columns=['Default_Rate (%)']).copy()\n",
    "\n",
    "# IMPORTANT: For a radar chart, we must scale all values to a common range, like 0-1\n",
    "scaler = MinMaxScaler()\n",
    "radar_data_scaled = pd.DataFrame(scaler.fit_transform(radar_data), columns=radar_data.columns, index=radar_data.index)\n",
    "\n",
    "# Create an interactive radar chart with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in radar_data_scaled.index:\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=radar_data_scaled.loc[i].values,\n",
    "        theta=radar_data_scaled.columns,\n",
    "        fill='toself',\n",
    "        name=f'Segment {i}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 1]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "  title=\"Comparative Profile of Segments (Radar Chart)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555c606-fa8d-4fd8-9559-67c3f542c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is very large, let's take a sample for better readability of the plot\n",
    "data_sample = data.sample(n=10000, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "sns.scatterplot(\n",
    "    data=data_sample,\n",
    "    x='CREDIT_TO_INCOME_RATIO', \n",
    "    y='YEARS_EMPLOYED', \n",
    "    hue='Segment',       # Color the points by segment\n",
    "    palette='viridis',\n",
    "    alpha=0.7,\n",
    "    s=50                 # Point size\n",
    ")\n",
    "\n",
    "plt.title('Segment Distribution by the Two Most Differentiating Features', fontsize=16)\n",
    "plt.xlabel('Credit-to-Income Ratio (Indebtedness)', fontsize=12)\n",
    "plt.ylabel('Years Employed', fontsize=12)\n",
    "plt.legend(title='Segment')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b87234-3d01-42ea-a61f-8e6344749a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export artifacts for LR & Streamlit ===\n",
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "from joblib import dump\n",
    "import sklearn, sys\n",
    "\n",
    "art = Path(\"artifacts\"); art.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) Find the processed X that was used for training (and convert to dense)\n",
    "X_proc = None\n",
    "for name in [\"processed_data_winsorized\", \"processed_trim\", \"processed_data\"]:\n",
    "    if name in globals():\n",
    "        X_proc = globals()[name]\n",
    "        break\n",
    "assert X_proc is not None, \"Processed data (processed_*) not found.\"\n",
    "\n",
    "if hasattr(X_proc, \"toarray\"):\n",
    "    X_proc = X_proc.toarray()\n",
    "X_proc = np.asarray(X_proc, dtype=float)\n",
    "\n",
    "# 2) Winsorization boundaries (calculate from training data if not already available)\n",
    "# Note: winsor_cols and quantiles are assumed to be 0.1%-99.9%\n",
    "if \"winsor_cols\" not in globals():\n",
    "    winsor_cols = [\n",
    "        'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','CREDIT_TO_INCOME_RATIO',\n",
    "        'ANNUITY_TO_INCOME_RATIO','CREDIT_TERM','AGE_YEARS','YEARS_EMPLOYED'\n",
    "    ]\n",
    "if \"q_low\" in globals() and \"q_hi\" in globals():\n",
    "    q_low_dict = {k: float(v) for k, v in q_low.to_dict().items() if k in winsor_cols}\n",
    "    q_hi_dict  = {k: float(v) for k, v in q_hi.to_dict().items()  if k in winsor_cols}\n",
    "else:\n",
    "    assert \"data_for_model\" in globals(), \"data_for_model not found (required to derive winsor boundaries).\"\n",
    "    q_low_dict = {c: float(data_for_model[c].quantile(0.001)) for c in winsor_cols if c in data_for_model.columns}\n",
    "    q_hi_dict  = {c: float(data_for_model[c].quantile(0.999)) for c in winsor_cols if c in data_for_model.columns}\n",
    "\n",
    "# 3) Input schema (the columns the model expects)\n",
    "if \"data_for_model_winsorized\" in globals():\n",
    "    model_input_cols = list(data_for_model_winsorized.columns)\n",
    "elif \"data_for_model\" in globals():\n",
    "    model_input_cols = list(data_for_model.columns)\n",
    "else:\n",
    "    raise RuntimeError(\"Could not determine model input columns.\")\n",
    "\n",
    "# 4) Store categorical & numerical lists (useful for the Streamlit form)\n",
    "if \"categorical_features\" not in globals():\n",
    "    categorical_features = []\n",
    "if \"numeric_features\" not in globals():\n",
    "    numeric_features = []\n",
    "\n",
    "# 5) OneHotEncoder categories (optional, but good for consistency in production)\n",
    "try:\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    ohe_categories = {col: list(cats) for col, cats in zip(categorical_features, ohe.categories_)}\n",
    "except Exception:\n",
    "    ohe_categories = {}\n",
    "\n",
    "# 6) Config.json\n",
    "config = {\n",
    "    \"RANDOM_STATE\": int(globals().get(\"RANDOM_STATE\", 42)),\n",
    "    \"kmeans_k\": int(getattr(kmeans_final, \"n_clusters\", 3)),\n",
    "    \"winsor_cols\": winsor_cols,\n",
    "    \"winsor_low_quantile\": 0.001,\n",
    "    \"winsor_high_quantile\": 0.999,\n",
    "    \"winsor_low_bounds\": q_low_dict,\n",
    "    \"winsor_high_bounds\": q_hi_dict,\n",
    "    \"model_input_cols\": model_input_cols,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"ohe_categories\": ohe_categories,\n",
    "    \"versions\": {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"sklearn\": sklearn.__version__,\n",
    "    }\n",
    "}\n",
    "with open(art/\"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 7) Save the PREPROCESSOR and KMEANS used for training\n",
    "dump(preprocessor, art/\"preprocessor_kmeans.joblib\")\n",
    "dump(kmeans_final, art/\"kmeans_k3.joblib\")\n",
    "\n",
    "print(\"Artifacts saved to → ./artifacts/:\")\n",
    "print(\" - config.json\")\n",
    "print(\" - preprocessor_kmeans.joblib\")\n",
    "print(\" - kmeans_k3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32d96f-13b2-4337-b0d8-036f742d2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample from the 'data' DF, which contains segmented and engineered features\n",
    "(data[['CREDIT_TO_INCOME_RATIO','YEARS_EMPLOYED','Segment']]\n",
    " .dropna()\n",
    " .sample(n=10_000, random_state=42)\n",
    " .to_parquet('artifacts/scatter_sample.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990b7c5-8643-4697-b748-87a980c65191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (homecredit-310)",
   "language": "python",
   "name": "homecredit-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
